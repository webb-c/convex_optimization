{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 1st-order Wasserstein distance\n",
    "\n",
    "Let $X \\in \\mathcal{X}:=\\{-2,0,2\\}$ be a discrete random variable with probability distribution: $\\mathbb{P}_X(x)=\\frac{1}{4}, \\frac{1}{2}, \\frac{1}{4}$ for $x=-2,0,2$, respectively. Let $Y \\in \\mathcal{Y}:=\\{-4,-1,1,4\\}$ be another discrete random variable with: $\\mathbb{P}_Y(y)=\\frac{3}{8}, \\frac{1}{8}, \\frac{1}{8}, \\frac{3}{8}$ for $y=-4,-1,1,4$, respectively. Consider Monge's problem which can be formulated as follows. Given $\\mathbb{P}_X$ and $\\mathbb{P}_Y$,\n",
    "\n",
    "$$\n",
    "W\\left(\\mathbb{P}_X, \\mathbb{P}_Y\\right):=\\min _{\\mathbb{P}_{X, Y}} \\mathbb{E}[\\|X-Y\\|]\n",
    "$$\n",
    "\n",
    "where the minimization is over all joint distributions $\\mathbb{P}_{X, Y}$ respecting the marginals $\\mathbb{P}_X$ and $\\mathbb{P}_Y$ :\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\sum_{y \\in \\mathcal{Y}} \\mathbb{P}_{X, Y}(x, y)=\\mathbb{P}_X(x) & \\forall x \\in \\mathcal{X} \\\\\n",
    "\\sum_{x \\in \\mathcal{X}} \\mathbb{P}_{X, Y}(x, y)=\\mathbb{P}_Y(y) & \\forall y \\in \\mathcal{Y}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "(b) Solve the optimization problem using CVXPY.\n",
    "\n",
    "Hint: Given an optimization variable, say $z \\in \\mathbf{R}^d$, and $(w, A, b, C, e)$ that appear in the LP standard form, here is a sample script:\n",
    "\n",
    "```python\n",
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "z = cp.Variable(d)\n",
    "objective = cp.Minimize(np.transpose(w) @ z)\n",
    "constraints = [A@z-b <= 0, C@z-e == 0]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "prob.solve()\n",
    "\n",
    "print(\"status: \", prob.status)\n",
    "print(\"Optimal value: \", prob.value)\n",
    "print(\"Optimal var: \", z.value)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Linear classification\n",
    "\n",
    "Consider the linear classification problem wherein the goal is to find a boundary of the line form that can distinguish legitimate emails from spams. We are given $\\left\\{\\left(x^{(i)}, y^{(i)}\\right)\\right\\}_{i=1}^m$ training dataset (given in the file \"train.csv\" uploaded on the course website). Here $x^{(i)}:=\\left(x_1^{(i)}, x_2^{(i)}\\right)$ indicates a feature vector of the $i$ th example and $y^{(i)}$ denotes its corresponding label (legitimate $=+1$; spam $=-1$ ).\n",
    "\n",
    "\n",
    "(a) What are $m$ and dimension of $x^{(i)}$ ?\n",
    "\n",
    "Hint: You may want to use the following script:\n",
    "```python \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(’train.csv’)\n",
    "X = data[['X1', 'X2']].values\n",
    "y = data['Y'].values\n",
    "m,n = X.shape\n",
    "print(m,n)\n",
    "\n",
    "```\n",
    "\n",
    "(b) Use a Python code below to visualize the data points in the two-dimensional space.\n",
    "\n",
    "```python \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_y1 = X[y==+1] # legitimate\n",
    "X_y0 = X[y==-1] # spam\n",
    "\n",
    "plt.figure(figsize=(4,4), dpi=150)\n",
    "\n",
    "plt.scatter(X_y1[:,0],X_y1[:,1],\n",
    "            c=’blue’, label = ’legitimate’, marker=’o’)\n",
    "plt.scatter(X_y0[:,0],X_y0[:,1],\n",
    "            c=’red’, label = ’spam’, marker=’o’)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.title('Visualization of data points')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "(c) In class, we formulated a margin-based linear classifier as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min _{a, b, v^{(i)}} \\sum_{i=1}^m v(i): \\\\\n",
    "& \\quad y^{(i)}\\left(a^T x^{(i)}-b\\right)+v^{(i)} \\geq 1, \\quad i \\in\\{1, \\ldots, m\\} \\\\\n",
    "& \\quad v^{(i)} \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using a code below, print out the optimal solution.\n",
    "\n",
    "```python \n",
    "import cvxpy as cp\n",
    "\n",
    "a = cp.Variable(n)\n",
    "b = cp.Variable()\n",
    "v = cp.Variable(m)\n",
    "\n",
    "# constraints and the objective function\n",
    "constraints = [y[i]*(X[i,:]@a-b) + v[i]>=1 for i in range(m)] + [v>=0]\n",
    "objective = cp.Minimize(cp.sum(v))\n",
    "\n",
    "prob = cp.Problem(objective, constraints)\n",
    "prob.solve()\n",
    "\n",
    "print('status: ', prob.status)\n",
    "print('optimal_value: ', prob.value)\n",
    "print('optimal_var: ')\n",
    "\n",
    "print('a*: ',a.value)\n",
    "print('b*: ',b.value)\n",
    "print('v*: ',v.value)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Margin-based linear classifier vs. Least-Squares classifier \n",
    "\n",
    "Consider the legitimate-vs-spam email classification. We are given $\\{(x^{(i)}, y^{(i)})\\}^m_{i=1}$ training dataset (the same \"train.csv\" in Prob 5). In this problem, you will build up two classifiers: (i) the margin-based linear classifier; and (ii) the Least-Squares classifier (with a bias term). You will also compare the test error performances with test dataset $\\{(x^{(i)}_{\\text{test}}, y^{(i)}_{\\text{test}})\\}^{m_{\\text{test}}}_{i=1}$ (given in \"test.csv\" uploaded). You need the write a script for Python implementation.\n",
    "\n",
    "(b) Given the trained model $(a^*, b^*)$ in part (a), the margin-based linear classifier outputs:\n",
    "\n",
    "$$ y_{\\text {test }}^{(i)}=\\operatorname{sign}\\left(a^{* T} x_{\\text {test }}^{(i)}-b^*\\right)$$\n",
    "\n",
    "Using a Python code below, compute the test error of the margin-based linear classifier:\n",
    "\n",
    "```python \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "Xtest = test[['X1', 'X2']].values\n",
    "ytest = test['Y'].values\n",
    "m_test, n = Xtest.shape\n",
    "\n",
    "error_LP=np.sign(Xtest@a.value - b.value) != ytest\n",
    "print(\"Test error of the MB linear classifier: \",\n",
    "        sum(error_LP)/m_test)\n",
    "```\n",
    "\n",
    "(c) Formulate an optimization for the least-squares classifier. Using a Python code below, develop the LS classifier and print out $(w^*, c^*)$.\n",
    "\n",
    "```python \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "X = data[['X1', 'X2']].values\n",
    "y = data['Y'].values\n",
    "m,n = X.shape\n",
    "\n",
    "allone = np.ones((m,1))\n",
    "A = np.concatenate((X,allone),1)\n",
    "\n",
    "# closed form solution\n",
    "w_bar_star = np.dot(pinv(A),y)\n",
    "\n",
    "print('w*:', w_bar_star[0:2])\n",
    "print('c*:', w_bar_star[2])\n",
    "```\n",
    "\n",
    "(d) Double-check your answer in part (c) using the following CVXPY script:\n",
    "\n",
    "```python \n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "X = data[['X1', 'X2']].values\n",
    "y = data['Y'].values\n",
    "m,n = X.shape\n",
    "\n",
    "w = cp.Variable(n)\n",
    "c = cp.Variable()\n",
    "\n",
    "# constraints and the objective function\n",
    "objective = cp.Minimize( cp.sum((X@w+c -y)**2) )\n",
    "prob = cp.Problem(objective)\n",
    "prob.solve()\n",
    "\n",
    "print('status: ', prob.status)\n",
    "print('optimal value: ', prob.value)\n",
    "print('optimal var: ')\n",
    "print('w*: ',w.value)\n",
    "print('c*: ',c.value)\n",
    "```\n",
    "\n",
    "(e) Given the trained model $\\left(w^*, c^*\\right)$ in part $(c)$, the LS classifier outputs:\n",
    "$$\n",
    "y_{\\text {test }}^{(i)}=\\operatorname{sign}\\left(w^{* T} x_{\\text {test }}^{(i)}+c^*\\right)\n",
    "$$\n",
    "Using a Python code, compute the test error of the LS classifier. Which classifier is better between the LS classifier and the margin-based linear classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convex_optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
